<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Mushui Liu ï¼ˆåˆ˜æœ¨æ°´ï¼‰


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link
  defer
  rel="stylesheet"
  type="text/css"
  href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"
>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

<!-- Styles -->

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">




  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
<a href="https://scholar.google.com/citations?user=-WUyWpMAAAAJ&hl=en" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
<a href="https://github.com/XiaobuL" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>

        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          
          <!-- Other pages -->
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Mushui Liu
    </h1>
     <p class="desc">Zhejiang University</p>
  </header>

<article>

<div class="profile float-right">

<img class="img-fluid z-depth-1 rounded" src="" srcset="/assets/img/mushuiliu.jpg 120w">
    </div>
    <div class="clearfix">
      
      <p> Now I'm a researcher at Alibaba Group via the T-Star Lab Talent Program. I have received my Ph.D. degree in College of Information Science & Electronic Engineering from Zhejiang <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in June 2025, where I am supervised by Prof.Yu. My research is on AIGC, MLLM, Parameter-Efficient Fine-tuning (PEFT), Few-shot Learning (FSL), and Self-supervised Learning (SSL). I also received my B.S. degree (Microelectronics Science and Engineering) from <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in 2020.
      </p>
      <p>
        ğŸ’¬ Feel free to drop me emails (lms@zju.edu.cn) if you have an interest in the above topics, and remote cooperation is welcome.
      </p>
    </div>
      
  <div class="news">
  <h2>ğŸ”¥ News</h2>
    <div class="table-responsive" style="height: 250px; overflow-y:scroll">
      <table class="table table-sm table-borderless" style="width: 100%">
        <colgroup>
           <col span="1" style="width: 15%;">
           <col span="1" style="width: 85%;">
        </colgroup>
      <!-- Put <thead>, <tbody>, and <tr>'s here! -->
      <tbody>
        <tr>
          <th scope="row">May 19, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One paper </strong> is accepted to <strong>Knowledge-Based Systems</strong>.
          </td>
        </tr>
        
        <tr>
          <th scope="row">Apri 4, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One CVPR paper </strong> is selected as <strong>Highlight</strong>.
          </td>
        </tr>
        
        <tr>
          <th scope="row">Feb 27, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>CVPR-2025</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Jan 9, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>IEEE Transactions on Circuits and Systems for Video Technology</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Dec 20, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>Neural Networks</strong>.
          </td>
        </tr>
        <tr>
            <th scope="row">Dec 12, 2024</th>
            <td>
              ğŸ‰ğŸ‰ğŸ‰ <strong>Four</strong> papers are accepted to <strong>AAAI-2025</strong>.
            </td>
        </tr>
        <tr>
          <th scope="row">July 01, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>ECCV-2024</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">July 15, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>ACM MM-2024</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">July 12, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>ECAI-2024</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Feb 03, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>Neural Networks</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Oct 23, 2022</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>Neurocomputing</strong>.
          </td>
        </tr>
      </tbody>
      </table>
    </div>
  
</div>

<div class="publications">
<h2>ğŸ“ Selected Publications</h2>
<p> Full publication list can be found on <a href="https://scholar.google.com/citations?user=-WUyWpMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a>. <br>
<sup>*</sup>equal contribution; <sup>#</sup>corresponding author. </p>
  <h2 class="year">2024</h2>
  <ol class="bibliography">

<li>
<div class="row">
  <div class="col-md-3">
    <div class="img-fluid rounded">
      <img src="/assets/teaser/mars-aaai.jpg" alt="Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis" style="width: 100%;">
    </div>
  </div>
  <div id="mars-aaai-2025" class="col-md-9">
        <div class="title">
          <span>Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis</span>
          <!-- <span style="color:red;">(Oral)</span> -->
        </div>
      <div class="author">
            Wanggui He<sup>*</sup>, Siming Fu<sup>*</sup>, <em>Mushui Liu<sup>*</sup></em>, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, Hao Jiang
      </div>
      <div class="periodical">
        <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>
        2025      
      </div>
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      <a href="https://arxiv.org/pdf/2407.07614" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      <!-- <a href="https://xdimlab.github.io/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> -->
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Auto-regressive models have made significant progress in the realm of text-to-image synthesis, yet devising an appropriate model architecture and training strategy to achieve a satisfactory level remains an important avenue of exploration. In this work, we introduce MARS, a novel framework for T2I generation that incorporates a specially designed Semantic Vision-Language Integration Expert (SemVIE). This innovative component integrates pre-trained LLMs by independently processing linguistic and visual informationâ€”freezing the textual component while fine-tuning the visual component. This methodology preserves the NLP capabilities of LLMs while imbuing them with exceptional visual understanding. Building upon the powerful base of the pre-trained Qwen-7B, MARS stands out with its bilingual generative capabilities corresponding to both English and Chinese language prompts and the capacity for joint image and text generation.  The flexibility of this framework lends itself to migration towards \textbf{any-to-any} task adaptability. Furthermore, MARS employs a multi-stage training strategy that first establishes robust image-text alignment through complementary bidirectional tasks and subsequently concentrates on refining the T2I generation process, significantly augmenting text-image synchrony and the granularity of image details.  Notably, MARS requires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable results across a variety of benchmarks, illustrating the training efficiency and the potential for swift deployment in various applications. 
      </p>
    </div>
    
    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
    
<li>
  <div class="row">
    <div class="col-md-3">
      <div class="img-fluid rounded">
        <img src="/assets/teaser/llm4gen.jpg" alt="Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis" style="width: 100%;">
      </div>
    </div>
    <div id="llm4gen-aaai-2025" class="col-md-9">
          <div class="title">
            <span>LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image Generation</span>
            <!-- <span style="color:red;">(Oral)</span> -->
          </div>
        <div class="author">
          <em>Mushui Liu<sup>*</sup></em>, Yuhang Ma<sup>*</sup>, Zhen Yang, Jun Dan, Yunlong Yu, Zeng Zhao, Bai Liu, Changjie Fan, Zhipeng Hu
        </div>
        <div class="periodical">
          <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>
          2025      
        </div>
      <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
        
        <a href="https://arxiv.org/pdf/2407.00737" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
        <!-- <a href="https://xdimlab.github.io/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> -->
      </div>
  
      <!-- Hidden abstract block -->
      
      <div class="abstract hidden">
        <p>Diffusion models have exhibited substantial success in text-to-image generation. However, they often encounter challenges when dealing with complex and dense prompts involving multiple objects, attribute binding, and long descriptions. In this paper, we propose a novel framework called LLM4GEN, which enhances the semantic understanding of text-to-image diffusion models by leveraging the representation of Large Language Models (LLMs). It can be seamlessly incorporated into various diffusion models as a plug-and-play component. A specially designed Cross-Adapter Module (CAM) integrates the original text features of text-to-image models with LLM features, thereby enhancing text-to-image generation. Additionally, to facilitate and correct entity-attribute relationships in text prompts, we develop an entity-guided regularization loss to further improve generation performance. We also introduce DensePrompts, which contains 7,000 dense prompts to provide a comprehensive evaluation for the text-to-image generation task. Experiments indicate that LLM4GEN significantly improves the semantic alignment of SD1.5 and SDXL, demonstrating increases of 9.69% and 12.90% in color on T2I-CompBench, respectively. Moreover, it surpasses existing models in terms of sample quality, image-text alignment, and human evaluation. 
        </p>
      </div>
      
      <!-- Hidden bibtex block -->
      
    </div>
  </div>
</li>


<li>
  <div class="row">
    <div class="col-md-3">
      <div class="img-fluid rounded">
        <img src="/assets/teaser/ecer.jpg" alt="Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis" style="width: 100%;">
      </div>
    </div>
    <div id="ecer-fsl-aaai-2025" class="col-md-9">
          <div class="title">
            <span>Envisioning Class Entity Reasoning by Large Language Models for Few-shot Learning</span>
            <!-- <span style="color:red;">(Oral)</span> -->
          </div>
        <div class="author">
          <em>Mushui Liu</em>, Fangtai Wu, Bozheng Li, Ziqian Lu, Yunlong Yu, Xi Li
        </div>
        <div class="periodical">
          <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>
          2025      
        </div>
      <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
        
        <a href="https://arxiv.org/pdf/2408.12469" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
        <!-- <a href="https://xdimlab.github.io/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> -->
      </div>
  
      <!-- Hidden abstract block -->
      
      <div class="abstract hidden">
        <p>Few-shot learning (FSL) aims to recognize new concepts using a limited number of visual samples. Existing approaches
          attempt to incorporate semantic information into the limited
          visual data for category understanding. However, these methods often enrich class-level feature representations with abstract category names, failing to capture the nuanced features
          essential for effective generalization. To address this issue,
          we propose a novel framework for FSL, which incorporates
          both the abstract class semantics and the concrete class entities extracted from Large Language Models (LLMs), to enhance the representation of the class prototypes. Specifically,
          our framework composes a Semantic-guided Visual Pattern
          Extraction (SVPE) module and a Prototype-Calibration (PC)
          module, where the SVPE meticulously extracts semanticaware visual patterns across diverse scales, while the PC
          module seamlessly integrates these patterns to refine the visual prototype, enhancing its representativeness. Extensive
          experiments on four few-shot classification benchmarks and
          the BSCD-FSL cross-domain benchmarks showcase remarkable advancements over the current state-of-the-art methods.
          Notably, for the challenging one-shot setting, our approach,
          utilizing the ResNet-12 backbone, achieves an impressive average improvement of 1.95% over the second-best competitor.
        </p>
      </div>
      
      <!-- Hidden bibtex block -->
      
    </div>
  </div>
</li>


<li>
  <div class="row">
    <div class="col-md-3">
      <div class="img-fluid rounded">
        <img src="/assets/teaser/tsam.jpg" alt="Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis" style="width: 100%;">
      </div>
    </div>
    <div id="ecer-fsl-aaai-2025" class="col-md-9">
          <div class="title">
            <span>Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action Recognition</span>
            <!-- <span style="color:red;">(Oral)</span> -->
          </div>
        <div class="author">
          Bozheng Li<sup>*</sup>, <em>Mushui Liu<sup>*</sup></em>, Gaoang Wang, Yunlong Yu
        </div>
        <div class="periodical">
          <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>
          2025      
        </div>
      <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
        
        <a href="https://arxiv.org/pdf/2408.12475" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
        <!-- <a href="https://xdimlab.github.io/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> -->
      </div>
  
      <!-- Hidden abstract block -->
      
      <div class="abstract hidden">
        <p>In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for few-shot action recognition (FSAR), which incorporates a sequential perceiver adapter into the pre-training framework, to integrate both the spatial information and the sequential temporal dynamics into the feature embeddings. Different from the existing fine-tuning approaches that capture temporal information by exploring the relationships among all the frames, our perceiver-based
          adapter recurrently captures the sequential dynamics alongside the timeline, which could perceive the order change. To
          obtain the discriminative representations for each class, we
          extend a textual corpus for each class derived from the large
          language models (LLMs) and enrich the visual prototypes
          by integrating the contextual semantic information. Besides,
          We introduce an unbalanced optimal transport strategy for
          feature matching that mitigates the impact of class-unrelated
          features, thereby facilitating more effective decision-making.
          Experimental results on five FSAR datasets demonstrate that
          our method set a new benchmark, beating the second-best
          competitors with large margins.
        </p>
      </div>
      
      <!-- Hidden bibtex block -->
      
    </div>
  </div>
</li>


<li>
  <div class="row">
    <div class="col-md-3">
      <div class="img-fluid rounded">
        <img src="/assets/teaser/omniclip.jpg" alt="Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis" style="width: 100%;">
      </div>
    </div>
    <div id="ecer-fsl-aaai-2025" class="col-md-9">
          <div class="title">
            <span>OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning</span>
            <!-- <span style="color:red;">(Oral)</span> -->
          </div>
        <div class="author">
          <em>Mushui Liu</em>, Bozheng Li, Yunlong Yu
        </div>
        <div class="periodical">
          <em>In European Conference on Artifical Intelligence (ECAI)</em>
          2024
        </div>
      <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
        
        <a href="https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240499" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
        <!-- <a href="https://xdimlab.github.io/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> -->
      </div>
  
      <!-- Hidden abstract block -->
      
      <div class="abstract hidden">
        <p>
          Recent Vision-Language Models (VLMs) e.g. CLIP have made great progress in video recognition. Despite the improvement
          brought by the strong visual backbone in extracting spatial features,
          CLIP still falls short in capturing and integrating spatial-temporal
          features which is essential for video recognition. In this paper, we
          propose OmniCLIP, a framework that adapts CLIP for video recognition by focusing on learning comprehensive features encompassing
          spatial, temporal, and dynamic spatial-temporal scales, which we refer to as omni-scale features. This is achieved through the design
          of spatial-temporal blocks that include parallel temporal adapters
          (PTA), enabling efficient temporal modeling. Additionally, we introduce a self-prompt generator (SPG) module to capture dynamic
          object spatial features. The synergy between PTA and SPG allows
          OmniCLIP to discern varying spatial information across frames and
          assess object scales over time. We have conducted extensive experiments in supervised video recognition, few-shot video recognition,
          and zero-shot recognition tasks. The results demonstrate the effectiveness of our method, especially with OmniCLIP achieving a top-1
          accuracy of 74.30% on HMDB51 in a 16-shot setting, surpassing the
          recent MotionPrompt approach even with full training data.
        </p>
      </div>
      
      <!-- Hidden bibtex block -->
      
    </div>
  </div>
</li>

</div>

<h2 id="-internships">ğŸ“š Academic Services</h2>
<ul>
  <li><em>Conference: </em> ICLR, CVPR, ICCV, ACM'MM, BMVC.</li>
  <li><em>Journals: </em> TMM, TCSVT.</li>
</ul>

<h2 id="-internships">ğŸ’» Internships</h2>
<ul>
  <li><em>2024.06 - 2025, </em> Content AI, Alibaba Group, Hangzhou.</li>
  <li><em>2024.01 - 2024.05, </em> Fuxi AI Lab, NetEase, Hangzhou.</li>
  <li><em>2022, </em> Disney Hulu, Beijing.</li>
  <li><em>2022, </em> ByteDance, Beijing.</li>
</ul>

</article>

</div>


    </div>

<!-- Footer -->

<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2024 Mushui Liu.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
    Last updated: December 21, 2024.
    
  </div>
</footer>
</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

</html>
