<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mushui Liu åˆ˜æœ¨æ°´</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link
  defer
  rel="stylesheet"
  type="text/css"
  href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"
>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

<!-- Styles -->

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">




  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
<a href="https://scholar.google.com/citations?user=-WUyWpMAAAAJ&hl=en" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
<a href="https://github.com/XiaobuL" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>

        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          
          <!-- Other pages -->
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Mushui Liu åˆ˜æœ¨æ°´
    </h1>
  </header>

<article>

<table class="imgtable" style="width: 100%; border-collapse: collapse; margin: 20px 0;">
<tr>
<td style="width: 200px; vertical-align: top; padding-right: 20px;">
  <img src="/assets/img/mushuiliu.jpg" alt="Mushui Liu" style="width: 200px; height: auto; border-radius: 5px;" />
</td>
<td style="vertical-align: top; padding-left: 20px;" align="left">
  <p>
    <strong>Mushui Liu</strong><br />
    Researcher<br />
    <a href="https://www.alibaba.com/" target="_blank" rel="noopener noreferrer">Alibaba Group</a>, <a href="https://talent.taotian.com/star/home" target="_blank" rel="noopener noreferrer">T-Star Lab Talent Program</a><br />
    Email: lms@zju.edu.cn<br /><br />
    <a href="https://scholar.google.com/citations?user=-WUyWpMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a> / 
    <a href="https://github.com/XiaobuL" target="_blank" rel="noopener noreferrer">GitHub</a>
  </p>
</td>
</tr>
</table>

<h2>About me</h2>
<p>
  I am currently a Researcher at <a href="https://www.alibaba.com/" target="_blank" rel="noopener noreferrer">Alibaba Group</a> via the <a href="https://talent.taotian.com/star/home" target="_blank" rel="noopener noreferrer">T-Star Lab Talent Program</a>. I received my Ph.D. degree in the College of Information Science & Electronic Engineering from <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in June 2025, where I was supervised by <a href="https://person.zju.edu.cn/en/yunlong" target="_blank" rel="noopener noreferrer">Prof. Yunlong Yu</a>. During my Ph.D. period, I worked closely with <a href="https://labazh.github.io/" target="_blank" rel="noopener noreferrer">Bozheng Li</a>, <a href="https://yuhang-ma.github.io/" target="_blank" rel="noopener noreferrer">Yuhang Ma</a>, <a href="https://scholar.google.com/citations?user=Vm1moSIAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Zhen Yang</a>, <a href="https://scholar.google.com/citations?user=beobICAAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Wanggui He</a>, and <a href="https://scholar.google.com/citations?user=tql_Zc4AAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Dr. Siming Fu</a>. I also received my B.S. degree (Microelectronics Science and Engineering) from <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in 2020.
</p>

<p>
  My research focuses on advancing the frontiers of multimodal artificial intelligence, with particular emphasis on <strong>Image/Video Generation</strong>, <strong>Video Understanding</strong>, <strong>Unified Models</strong>, and <strong>Representation Learning</strong>. 
</p>

<p>
  ğŸ’¬ Our team is hiring research interns which have strong engineering skills and a strong interest in AIGC. Feel free to drop me an email (lms@zju.edu.cn) if you have an interest in the above topics, and remote cooperation is welcome.
</p>
      
  <div class="news">
  <h2>ğŸ”¥ News</h2>
    <div class="table-responsive" style="height: 250px; overflow-y:scroll">
      <table class="table table-sm table-borderless" style="width: 100%">
        <colgroup>
           <col span="1" style="width: 15%;">
           <col span="1" style="width: 85%;">
        </colgroup>
      <!-- Put <thead>, <tbody>, and <tr>'s here! -->
      <tbody>
        <tr>
          <th scope="row">Nov 08, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>Two paper </strong> is accepted to <strong>AAAI-2026</strong>.
          </td>
        </tr>
        
        <tr>
          <th scope="row">Sep 27, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One paper </strong> is accepted to <strong>Neurocomputing</strong>.
          </td>
        </tr>
        
        <tr>
          <th scope="row">May 19, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One paper </strong> is accepted to <strong>Knowledge-Based Systems</strong>.
          </td>
        </tr>
        
        <tr>
          <th scope="row">Apri 4, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One CVPR paper </strong> is selected as <strong>Highlight</strong>.
          </td>
        </tr>
        
        <tr>
          <th scope="row">Feb 27, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>CVPR-2025</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Jan 9, 2025</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>IEEE Transactions on Circuits and Systems for Video Technology</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Dec 20, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>Neural Networks</strong>.
          </td>
        </tr>
        <tr>
            <th scope="row">Dec 12, 2024</th>
            <td>
              ğŸ‰ğŸ‰ğŸ‰ <strong>Four</strong> papers are accepted to <strong>AAAI-2025</strong>.
            </td>
        </tr>
        <tr>
          <th scope="row">July 01, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>ECCV-2024</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">July 15, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>ACM MM-2024</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">July 12, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>ECAI-2024</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Feb 03, 2024</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>Neural Networks</strong>.
          </td>
        </tr>
        <tr>
          <th scope="row">Oct 23, 2022</th>
          <td>
            ğŸ‰ğŸ‰ğŸ‰ <strong>One</strong> paper is accepted to <strong>Neurocomputing</strong>.
          </td>
        </tr>
      </tbody>
      </table>
    </div>
  
</div>

<div class="publications">
<h2>ğŸ“ Selected Publications</h2>
<p> Full publication list can be found on <a href="https://scholar.google.com/citations?user=-WUyWpMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a>. <br>
<sup>*</sup>equal contribution; <sup>#</sup>corresponding author. </p>

  <!-- Image Generation -->
  <h3 style="margin-top: 2rem; margin-bottom: 1rem; color: var(--global-theme-color); border-bottom: 2px solid var(--global-theme-color); padding-bottom: 0.5rem;">1. Image Generation</h3>
  <ol>

<li id="tfcustom-cvpr-2025">
  <a href="http://openaccess.thecvf.com/content/CVPR2025/html/Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance_CVPR_2025_paper.html">TFCustom: Customized Image Generation with Time-Aware Frequency Feature Guidance</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Dong She, Jingxuan Pang, Qihan Huang, Jiacheng Ying, Wanggui He, Yuanlei Hou, Siming Fu <br />
  <i>IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR, Highlight â­â­â­)</strong> </i>, 2025.
</li>

<li id="llm4gen-aaai-2025">
  <a href="https://arxiv.org/pdf/2407.00737">LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image Generation</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Yuhang Ma<sup>*</sup>, Zhen Yang, Jun Dan, Yunlong Yu, Zeng Zhao, Bai Liu, Changjie Fan, Zhipeng Hu <br />
  <i>The 39th Annual AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong></i>. 2025.
</li>

<li id="coar-2025">
  <a href="https://arxiv.org/pdf/2508.07341">CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation</a> <br />
  Fangtai Wu<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Weijie He, Wanggui He, Hao Jiang, Zhao Wang, Yunlong Yu <br />
  <i>In Submission</i>.
</li>

<li id="mosaic-2025">
  <a href="https://arxiv.org/abs/2509.01977">MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement</a> <br />
  Dong She<sup>*</sup>, Siming Fu<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Qiaoqiao Jin, Hualiang Wang, Mu Liu, Jidong Jiang <br />
  <i>In Submission</i>.
</li>

<li id="restorerid-2024">
  <a href="https://arxiv.org/pdf/2411.14125">RestorerID: Towards Tuning-Free Face Restoration with ID Preservation</a> <br />
  Jiacheng Ying<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Zhaoyang Wu, Rui Zhang, Zhelun Yu, Siming Fu, Shiyu Cao, Chen Wu, Yunlong Yu, Hailin Shen <br />
  <i>In Submission</i>.
</li>

<li id="rectifiedhr-2025">
  <a href="https://ui.adsabs.harvard.edu/abs/2025arXiv250302537Y/abstract">RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification</a> <br />
  Zhen Yang, Guibao Shen, Liang Hou, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Luozhou Wang, Xin Tao, Pengfei Wan, Di Zhang, Ying-Cong Chen <br />
  <i>In Submission</i>.
</li>

</ol>

  <!-- Unified Models -->
  <h3 style="margin-top: 2rem; margin-bottom: 1rem; color: var(--global-theme-color); border-bottom: 2px solid var(--global-theme-color); padding-bottom: 0.5rem;">2. Unified Models</h3>
  <ol>

<li id="fuse-aaai-2026">
  FUSE: Fine-Grained and Semantic-Aware Learning for Unified Image Understanding and Generation <br />
  Peng Zhang<sup>*</sup>, Wanggui He<sup>*</sup><sup>â€ </sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Wenyi Xiao, Siyu Zou, Yuan Li, Xingjian Wang, Guanghao Zhang, Yanpeng Liu, Weilong Dai, Jinlong Liu, Shuyi Ying, Ruikai Zhou, Yunlong Yu, Yubo Tao, Hai Lin<sup>#</sup>, Hao Jiang<sup>#</sup> <br />
  <i>The 40th Annual AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong></i>. 2026.
</li>

<li id="mars-aaai-2025">
  <a href="https://arxiv.org/pdf/2407.07614">Mars: Mixture of Auto-Regressive Models for Fine-grained Text-to-Image Synthesis</a> <br />
  Wanggui He<sup>*</sup>, Siming Fu<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, Hao Jiang <br />
  <i>The 39th Annual AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong></i>. 2025.
</li>

<li id="mint-2025">
  <a href="https://arxiv.org/pdf/2503.01298">Mint: Multi-Modal Chain of Thought in Unified Generative Models for Enhanced Image Generation</a> <br />
  Yi Wang<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Wanggui He<sup>*</sup>, Lei Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Yubo Tao, ... <br />
  <i>In Submission</i>.
</li>

</ol>

  <!-- Video Understanding & Generation -->
  <h3 style="margin-top: 2rem; margin-bottom: 1rem; color: var(--global-theme-color); border-bottom: 2px solid var(--global-theme-color); padding-bottom: 0.5rem;">3. Video Understanding & Generation</h3>
  <ol>

<li id="cmmcot-aaai-2026">
  <a href="/assets/teaser/CMMCoT.pdf">CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal Chain-of-Thought and Memory Augmentation</a> <br />
  Guozhen Zhang<sup>*</sup>, Tianyu Zhong<sup>*</sup>, Yuxuan Xia<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Zhelun Yu, Haoyuan Li, Wanggui He, Fangxun Shu, Dong She, ... <br />
  <i>The 40th Annual AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong></i>. 2026.
</li>

<li id="tsam-aaai-2025">
  <a href="https://arxiv.org/pdf/2408.12475">Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action Recognition</a> <br />
  Bozheng Li<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Gaoang Wang, Yunlong Yu <br />
  <i>The 39th Annual AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong></i>. 2025.
</li>

<li id="omniclip-ecai-2024">
  <a href="https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240499">OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Bozheng Li, Yunlong Yu <br />
  <i>European Conference on Artificial Intelligence <strong>(ECAI)</strong></i>. 2024.
</li>

<li id="dyst-xl-2025">
  <a href="https://arxiv.org/pdf/2504.15032">DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Weijie He<sup>*</sup>, Yunlong Yu, Zhao Wang, Chao Wu <br />
  <i>In Submission</i>.
</li>

<li id="customvideox-2025">
  <a href="https://arxiv.org/abs/2502.06527v1">CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers</a> <br />
  Dong She<sup>*</sup>, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span><sup>*</sup>, Jingxuan Pang, Jiacheng Wang, Zhen Yang, Wanggui He, Guanghao Zhang, Yi Wang, ... <br />
  <i>In Submission</i>.
</li>

</ol>

  <!-- Representation Learning -->
  <h3 style="margin-top: 2rem; margin-bottom: 1rem; color: var(--global-theme-color); border-bottom: 2px solid var(--global-theme-color); padding-bottom: 0.5rem;">4. Representation Learning</h3>
  <ol>

<li id="ecer-fsl-aaai-2025">
  <a href="https://arxiv.org/pdf/2408.12469">Envisioning Class Entity Reasoning by Large Language Models for Few-shot Learning</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Fangtai Wu, Bozheng Li, Ziqian Lu, Yunlong Yu, Xi Li <br />
  <i>The 39th Annual AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong></i>. 2025.
</li>

<li id="improving-zeroshot-clip-2024">
  <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-72661-3_19.pdf">Improving Zero-Shot Generalization for CLIP with Variational Adapter</a> <br />
  Ziqian Lu, Fangtai Shen, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Yunlong Yu, Xi Li <br />
  <i>European Conference on Computer Vision <strong>(ECCV)</strong></i>. 2024.
</li>

<li id="variational-adapter-tcsvt-2025">
  <a href="https://ieeexplore.ieee.org/abstract/document/10843280/">Variational Adapter: Improving CLIP in Data-Imbalanced Scenarios</a> <br />
  Ziqian Lu, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Yunlong Yu, Zhaoyang Wang, Xi Li, Jie Han <br />
  <i>IEEE Transactions on Circuits and Systems for Video Technology <strong>(IEEE TCSVT)</strong></i>. 2025.
</li>

<li id="synth-clip-2025">
  <a href="https://www.sciencedirect.com/science/article/pii/S0893608024010128">Synth-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited Scenarios</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Wanggui He, Ziqian Lu, Jun Dan, Yunlong Yu, Yuhang Li, Xi Li, Jie Han <br />
  <i>Neural Networks <strong>(Neural Networks)</strong></i>. 2025.
</li>

<li id="fully-finetuned-clip-2025">
  <a href="https://www.sciencedirect.com/science/article/pii/S0950705125008652">Fully Fine-Tuned CLIP Models are Efficient Few-Shot Learners</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Bozheng Li, Jun Dan, Ziqian Lu, Zhaoyang Wang, Yunlong Yu <br />
  <i>Knowledge-Based Systems <strong>(KBS)</strong></i>. 2025.
</li>

<li id="tolerant-selfdistillation-2024">
  <a href="https://www.sciencedirect.com/science/article/pii/S0893608024001394">Tolerant Self-Distillation for Image Classification</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Yunlong Yu, Zhaoyang Ji, Jie Han, Zhaoyang Zhang <br />
  <i>Neural Networks <strong>(Neural Networks)</strong></i>. 2024.
</li>

<li id="hybrid-mask-2025">
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231225023604">Hybrid mask generation for infrared small target detection with single-point supervision</a> <br />
  Weijie He, <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Yunlong Yu <br />
  <i>Neurocomputing <strong>(Neurocomputing)</strong></i>. 2025.
</li>

<li id="mimo-wnet-2023">
  <a href="https://www.sciencedirect.com/science/article/pii/S0925231222012954">Lightweight MIMO-WNet for single image deblurring</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Yunlong Yu, Yingming Li, Zhong Ji, Wen Chen, Yang Peng <br />
  <i>Neurocomputing <strong>(Neurocomputing)</strong></i>, Vol. 516, pp. 106-114. 2023.
</li>

<li id="cm-unet-2024">
  <a href="https://arxiv.org/pdf/2405.10530">CM-UNet: Hybrid CNN-Mamba UNet for Remote Sensing Image Semantic Segmentation</a> <br />
  <span style="color:#b02418; font-weight:bold;">Mushui Liu</span>, Jun Dan, Ziqian Lu, Yunlong Yu, Yuhang Li, Xi Li <br />
  <i>In Submission</i>.
</li>

</ol>

</div>

<h2 id="-internships">ğŸ“š Academic Services</h2>
<ul>
  <li><em>Conference: </em> ICLR, CVPR, AAAI, ACM'MM, BMVC.</li>
  <li><em>Journals: </em> TMM, TCSVT, KBS.</li>
</ul>

<h2 id="-internships">ğŸ’» Internships</h2>
<ul>
  <li><em>2024.06 - 2025, </em> Content AI, Alibaba Group, Hangzhou.</li>
  <li><em>2024.01 - 2024.05, </em> Fuxi AI Lab, NetEase, Hangzhou.</li>
  <li><em>2022, </em> Disney Hulu, Beijing.</li>
  <li><em>2022, </em> ByteDance, Beijing.</li>
</ul>

</article>

<!-- MapMyVisitors Visitor Statistics -->
<div style="text-align: center; margin: 20px 0;">
<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=MUMqYmZi9dD6sWA6QkWRAX2-1UMsbQaVsidh4cA0BHg&cl=ffffff&w=a"></script>
</div>

</div>


    </div>

</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/XiaobuL/xiaobul.github.io/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>

</html>